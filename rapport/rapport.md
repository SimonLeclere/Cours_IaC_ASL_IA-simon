---
Simon Leclere
2026
---

# TP terraform & kubernetes - Partie 1

Dans cette premi√®re partie, on va utiliser les scripts et la configuration fournie pour d√©ployer un cluster Kubernetes sur Proxmox via Terraform et Ansible.

Rappel des termes :
- **Proxmox VE** : hyperviseur de virtualisation utilis√© pour h√©berger les machines virtuelles.
- **Template** : image VM pr√©configur√©e utilis√©e pour cloner des VMs identiques.
- **Terraform** : outil d'IaC utilis√© pour cr√©er les VMs et les ressources dans Proxmox via un provider.
- **Ansible** : outil d'orchestration pour configurer les nodes (installation de containerd, kubeadm, kubelet, kubectl, etc.).
- **Kubernetes**: orchestrateur de conteneurs pour d√©ployer et g√©rer des applications conteneuris√©es.
- **kubeadm / kubelet / kubectl** : composants Kubernetes pour initialiser et g√©rer le cluster.

## Etape 1: Cr√©ation du template de VM

Avant de commencer le TP, on a d√©j√† une installation proxmox fonctionnelle, avec un acc√®s root via SSH configur√©, un stockage NFS mont√© et deux bridges vmbr0 et vmbr1.

Dans un premier temps, on commence par pr√©parer l'image de base, qui sera plus tard clon√©e par terraform pour cr√©er les machines. Cela permet de partir d'une image propre et reproductible, avec les paquets et l'agent cloud-init correctement configur√©s.

Pour √ßa, on ex√©cute le script `k8s-proxmox-iac/scripts/create_vm_template.sh`, qui pr√©pare et convertit une VM Ubuntu Noble 24.04 LTS en template pour Proxmox VE (installation des paquets utiles, configuration SSH, nettoyage des logs et des identifiants uniques). Le template sert ensuite de source pour cloner les machines lors du d√©ploiement Terraform.

![Sortie du script de cr√©ation de template](assets/script_create_vm_template.png)

## Etape 2: D√©ploiement Terraform

La configuration Terraform se trouve dans `k8s-proxmox-iac/terraform` :
- `providers.tf` contient la configuration du provider Proxmox,
- `variables.tf` et `terraform.tfvars` d√©finissent le nombre de nodes, ressources et adresses r√©seau,
- un module `modules/k8s-node` permet de factoriser la cr√©ation des VMs.

Avant de le provisionning terraform, quelques changements sont quand m√™me n√©cessaires. Notamment dans `terraform.tfvars` pour adapter les param√®tres r√©seau √† notre environnement Proxmox. Aussi, on change de version pour le provider Proxmox dans `providers.tf` pour √©viter des probl√®mes de compatibilit√©.

On applique la configuration avec les commandes suivantes:
```bash
terraform init # initialise le backend et t√©l√©charge les plugins
terraform plan # affiche le plan d'ex√©cution
terraform apply # applique la configuration et cr√©e les ressources
```

![alt text](assets/tfapply.png)

Apr√®s `apply`, Terraform retourne des outputs (IP des machines, IDs, etc.) utilisables pour l'inventaire Ansible.

On peut √©galement voir les VMs cr√©√©es dans l'interface Proxmox :
![Etat de proxmox apr√®s le provisionning via Terraform](assets/proxmox.png)

## Etape 3: Configuration via Ansible

Maintenant qu'on a l'infrastructure de base (les VMs) cr√©√©e par Terraform, on utilise Ansible pour configurer les nodes et installer Kubernetes. Pour ca, on dispose de 4 playbooks dans `k8s-proxmox-iac/ansible/playbooks`, qui seront ex√©cut√©s dans l'ordre.

Avant de les ex√©cuter, on s'assure que notre machine locale peut se connecter aux VMs via SSH en utilisant la cl√© priv√©e g√©n√©r√©e par le script de cr√©ation de template (situ√©e dans `k8s-proxmox-iac/scripts/id_rsa_proxmox_templates`). Si besoin, on cr√©e un tunnel SSH pour acc√©der aux VMs si elles ne sont pas directement accessibles.

### `01-prepare-nodes.yml` : pr√©paration des nodes (utilisateurs, SSH, d√©pendances syst√®me)

![alt text](assets/playbook01.png)

### `02-install-k8s.yml` : installation de `containerd`, `kubeadm`, `kubelet`, `kubectl` et verrouillage des versions

![alt text](assets/playbook02.png)

### `03-init-master.yml` : initialisation du master avec `kubeadm init`, configuration de `~/.kube/config`, installation du r√©seau Pod (ici Flannel via `kube-flannel.yml`) et g√©n√©ration de la commande de join qui est enregistr√©e dans `/tmp/k8s_join_command.sh`,

![alt text](assets/playbook03.png)

### `04-join-workers.yml` : ex√©cution de la commande de jonction sur les workers pour les ajouter au cluster.

![alt text](assets/playbook04.png)

## Etape 4: V√©rifications et conclusion

Apr√®s l'ex√©cution des playbooks, on devrait normalement avoir un cluster Kubernetes fonctionnel avec un master et 3 workers. Pour v√©rifier l'√©tat du cluster, on peut se connecter au master et ex√©cuter quelques commandes :
- `kubectl get nodes` -> v√©rifier que tous les nodes apparaissent en `Ready`
![alt text](assets/kubectlnodes.png)

- `kubectl get pods -A` -> v√©rifier que les pods du plan de contr√¥le et du CNI (Flannel) sont en `Running`

![alt text](assets/kubectlpods.png)

Pour la suite du TP, on simplifiera les commandes en copiant le fichier kubeconfig du master vers notre poste local, ce qui nous permettra d'utiliser `kubectl` directement depuis notre machine.

# Partie 2 : Exercices

On dispose d'un dossier `Exercises` contenant plusieurs exercices pratiques pour se familiariser avec Kubernetes : services, volumes persistants, ConfigMaps, d√©ploiement d'une application PHP connect√©e √† MariaDB, etc.

Pour certains, quelques questions de r√©flexion sont pos√©es dans les fichiers README.md.

## Exercice 01 - D√©ployer une application de test sur Kubernetes

Objectif : D√©ployer une application simple (`nginx`) sur un cluster Kubernetes afin de v√©rifier le bon fonctionnement du cluster, du r√©seau et des services.

On cr√©e un d√©ploiement NGINX avec 3 r√©plicas, on expose le d√©ploiement via un Service de type NodePort, puis on teste l'acc√®s √† l'application.

```bash
# On cr√©e un d√©ploiement nginx avec 3 replicas
kubectl create deployment nginx --image=nginx --replicas=3

# On v√©rifie que les pods ont bien √©t√© cr√©√©s
kubectl get pods

# On expose le d√©ploiement via un NodePort
kubectl expose deployment nginx --port=80 --type=NodePort

# On r√©cup√®re les informations du service (port NodePort attribu√©)
kubectl get svc nginx

# Enfin, on teste l'acc√®s √† l'application depuis l'ext√©rieur du cluster
curl http://10.0.0.11:31711
```

On peut voir les pods nginx en √©tat Running et le service expos√© en NodePort:
![alt text](assets/exercice01-1.png)

Si on ouvre l'url, on voit bien la page d'accueil NGINX s'afficher:
![alt text](assets/exercise01.png)

### Questions:

- Sur quels n≈ìuds les pods nginx sont-ils d√©ploy√©s ?
> Les pods nginx sont d√©ploy√©s sur les n≈ìuds workers du cluster Kubernetes. On peut v√©rifier cela en utilisant la commande `kubectl get pods -o wide`, qui affiche le n≈ìud sur lequel chaque pod est h√©berg√©.

- Que se passe-t-il si vous supprimez un pod nginx ?
> Si un pod nginx est supprim√©, Kubernetes d√©tecte que le nombre de r√©plicas sp√©cifi√© dans le d√©ploiement n'est plus respect√©. En cons√©quence, il cr√©e automatiquement un nouveau pod pour remplacer celui qui a √©t√© supprim√©, assurant ainsi que le nombre de r√©plicas reste constant √† 3.

- Quelle est la diff√©rence entre un Service ClusterIP et NodePort ?
> Un Service ClusterIP est le type de service par d√©faut dans Kubernetes. Il expose le service uniquement √† l'int√©rieur du cluster, permettant aux pods de communiquer entre eux via une adresse IP interne. En revanche, un Service NodePort expose le service sur un port sp√©cifique de chaque n≈ìud du cluster, permettant l'acc√®s au service depuis l'ext√©rieur du cluster en utilisant l'adresse IP du n≈ìud et le port NodePort attribu√©.

##¬†Exercice 02 - Mise √† l‚Äô√©chelle (Scaling) d‚Äôune application

Objectif : Apprendre √† mettre √† l‚Äô√©chelle un d√©ploiement Kubernetes en modifiant le nombre de r√©plicas d‚Äôune application, et observer comment le cluster r√©agit automatiquement.

Contexte : Dans l‚Äôexercice pr√©c√©dent, on a d√©ploy√© un d√©ploiement `nginx` avec 3 r√©plicas.

L‚Äôobjectif maintenant est de :
- Augmenter le nombre de pods pour g√©rer plus de trafic
- R√©duire le nombre de pods si n√©cessaire
- Observer la r√©action du cluster

On voit sur le screenshot ci-dessous l'augmentation automatique du nombre de pods de 3 √† 5 par la commande `kubectl scale`.
![alt text](assets/exercise02.png)

Enfin, on supprime un pod au hasard pour observer que Kubernetes recr√©e automatiquement un nouveau pod pour maintenir le nombre de r√©plicas d√©fini:

![alt text](assets/exercise02-1.png)

## Exercice 03 - Services Kubernetes et acc√®s r√©seau

Objectif : Comprendre comment Kubernetes expose une application √† l‚Äôint√©rieur et √† l‚Äôext√©rieur du cluster en utilisant diff√©rents types de Services (`ClusterIP`, `NodePort`).

Maintenant que l'application NGINX est d√©ploy√©e, on va cr√©er un Service de type ClusterIP pour l'acc√®s interne, puis un Service de type NodePort pour l'acc√®s externe.

1. Cr√©ation d‚Äôun Service ClusterIP (interne) :
![alt text](assets/exercise03.png)

On remarque que le Service ClusterIP n‚Äôa pas d‚ÄôEXTERNAL-IP, car il est uniquement accessible √† l‚Äôint√©rieur du cluster.

On peut √©galement tester l‚Äôacc√®s interne depuis un pod en cr√©ant un pod temporaire avec l‚Äôimage alpine et en utilisant curl (ou wget) pour acc√©der au service NGINX via son nom DNS.

![alt text](assets/exercise03-1.png)

2. Cr√©ation d‚Äôun Service NodePort (externe) :
![alt text](assets/exercise03-3.png)

On remarque que le Service NodePort attribue un port externe (ici 31078) qui permet d‚Äôacc√©der √† l‚Äôapplication depuis l‚Äôext√©rieur du cluster. Par exemple, firefox sur la machine h√¥te avec l‚ÄôIP du n≈ìud et le port NodePort.

![alt text](assets/exercise03-2.png)

## Exercice 04 - ConfigMaps & Secrets

Objectif : Apprendre √† g√©rer la configuration et les secrets dans Kubernetes afin que les applications restent d√©coupl√©es de leur configuration et que les informations sensibles soient prot√©g√©es.

Pour stocker des informations (configuration, mots de passe, cl√©s API) dans kubernetes, il existe deux objets principaux :
- ConfigMap : pour les informations non sensibles
- Secret : pour les informations sensibles

Dans cet exercice, on va tester les deux options pour stocker un message de bienvenue pour NGINX (ConfigMap) et un mot de passe (Secret).

1. Cr√©ation d‚Äôun ConfigMap

Cr√©ation du ConfigMap avec un message de bienvenue :
![alt text](assets/exercise04.png)

Ensuite, on cr√©e un pod qui utilise le ConfigMap pour injecter la variable d‚Äôenvironnement WELCOME_MESSAGE :
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-config-test
spec:
  containers:
  - name: nginx
    image: nginx
    env:
    - name: WELCOME_MESSAGE
      valueFrom:
        configMapKeyRef:
          name: nginx-config
          key: welcome_message
```

Puis on d√©ploie le pod et on v√©rifie que la variable d‚Äôenvironnement est bien d√©finie :
![alt text](assets/exercise04-1.png)

2. Cr√©ation d‚Äôun Secret

Cr√©ation du Secret avec un mot de passe encod√© en base64 :
![alt text](assets/exercise04-2.png)

Ensuite, on cr√©e un pod qui utilise le Secret pour injecter la variable d‚Äôenvironnement DB_PASSWORD :
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-secret-test
spec:
  containers:
  - name: nginx
    image: nginx
    env:
    - name: DB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: db-secret
          key: password
```

Puis on d√©ploie le pod et on v√©rifie que la variable d‚Äôenvironnement est bien d√©finie :
![alt text](assets/exercise04-3.png)

##¬†Exercice 05 - Rolling Update

Objectif : Apprendre √† mettre √† jour un d√©ploiement Kubernetes sans interruption gr√¢ce aux rolling updates.

Les applications doivent souvent √™tre mises √† jour sans interrompre le service. Kubernetes permet de mettre √† jour les images d‚Äôun Deployment progressivement, un pod apr√®s l‚Äôautre.

Dans cet exercice, on va mettre √† jour l‚Äôimage NGINX d‚Äôun d√©ploiement existant vers une version plus r√©cente, tout en assurant une disponibilit√© continue.

1. V√©rification du d√©ploiement existant
![alt text](assets/exercise05.png)

2. Mise √† jour de l‚Äôimage du d√©ploiement et suivi de la mise √† jour
![alt text](assets/exercise05-1.png)

3. Rollback vers l'ancienne version
![alt text](assets/exercise05-2.png)

On peut voir que les pods sont mis √† jour progressivement, sans interruption de service. Kubernetes g√®re automatiquement le remplacement des pods et on peut revenir en arri√®re avec `rollout undo` si n√©cessaire.

## Exercice 06 - Ingress

Objectif : Comprendre comment exposer plusieurs services Kubernetes via un point d‚Äôentr√©e unique avec Ingress.

NodePort permet d‚Äôexposer un service par port, mais si on a plusieurs services, c‚Äôest compliqu√©. Ingress permet de router le trafic HTTP/S vers diff√©rents services en fonction de l‚ÄôURL ou du nom d‚Äôh√¥te.

Dans cet exercice, on va installer un contr√¥leur Ingress (NGINX), puis cr√©er un Ingress pour router le trafic vers un service NGINX. Enfin, on v√©rifiera que les requ√™tes HTTP sont correctement rout√©es par Ingress.

1. Installation du contr√¥leur Ingress (NGINX)

On d√©ploie les ressources n√©cessaires pour Ingress NGINX
![alt text](assets/exercise06-2.png)

Puis on v√©rifie que tout s'est bien cr√©√©
![alt text](assets/exercise06-3.png)

2. Cr√©ation d‚Äôun Ingress pour le service NGINX

On cr√©e le contr√¥leur Ingress NGINX :
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
spec:
  ingressClassName: nginx
  rules:
  - host: example.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: nginx
            port:
              number: 80
```

Puis on applique le manifest et on v√©rifie que l‚ÄôIngress est cr√©√© :
![alt text](assets/exercise06-1.png)

3. V√©rification du routage via Ingress

Pour tester le routage, on ajoute une entr√©e dans le fichier `/etc/hosts` de notre machine locale pour faire pointer `example.local` vers l‚ÄôIP du n≈ìud (r√©cup√©r√©e via `kubectl get nodes -o wide`).

Ensuite, on effectue une requ√™te HTTP vers `example.local` :
![alt text](assets/exercise06.png)

On peut voir que la requ√™te est bien rout√©e vers le service NGINX via Ingress.

##¬†Exercice 07 - Scaling & D√©ploiement avec Deployment

Objectif : Cr√©er un deployment √† partir d‚Äôun fichier YAML, comprendre le lien entre Deployment ‚Üí ReplicaSet ‚Üí Pods, et g√©rer le scaling.

Un Deployment permet de d√©clarer l‚Äô√©tat d√©sir√© d‚Äôune application (nombre de replicas, image, labels, etc.). Kubernetes s‚Äôoccupe de cr√©er et maintenir les pods correspondants. Les labels et le selector permettent de relier le Deployment aux pods qu‚Äôil g√®re.

Dans cet exercice, on va cr√©er un Deployment NGINX avec 3 r√©plicas, puis exposer le d√©ploiement via un Service NodePort, et enfin g√©rer le scaling en augmentant et diminuant le nombre de pods.

1. Cr√©ation du deployment

On commence par cr√©er un fichier web-deployment.yaml contenant :
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-logiciel # Le lien entre le deployment et les pods
  template:
    metadata:
      labels:
        app: nginx-logiciel # L'√©tiquette coll√©e sur chaque clone
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
```

Puis on le d√©ploie et on v√©rifie que les pods sont cr√©√©s :
![alt text](assets/exercise07.png)

2. Exposition du deployment via NodePort

On expose le d√©ploiement via un Service NodePort et on v√©rifie le service cr√©√© :
![alt text](assets/exercise07-1.png)

On teste l‚Äôacc√®s √† l‚Äôapplication :
![alt text](assets/exercise07-2.png)

3. Gestion du scaling

On augmente le nombre de pods √† 5 :
![alt text](assets/exercise07-3.png)

On diminue le nombre de pods √† 2 :
![alt text](assets/exercise07-4.png)

On peut voir que Kubernetes ajuste automatiquement le nombre de pods en fonction du nombre de r√©plicas sp√©cifi√© dans le Deployment.

## Exercice 08 - Service interne ClusterIP

Objectif: Apprendre √† exposer un Deployment uniquement √† l‚Äôint√©rieur du cluster avec un service de type ClusterIP, et v√©rifier que les pods du Deployment sont bien accessibles via ce service.

Dans cet exercice, on va cr√©er un service interne pour que les pods puissent communiquer entre eux ou avec d‚Äôautres pods sans exposer le service √† l‚Äôext√©rieur.

1. Cr√©ation du Service ClusterIP

On commence par cr√©er un fichier web-service.yaml avec le contenu suivant :
```yaml
apiVersion: v1
kind: Service
metadata:
  name: web-service-unique
spec:
  selector:
    app: nginx-logiciel # Il cherche les pods avec ce label (ceux de notre Deployment !)
  ports:
    - protocol: TCP
      port: 80          # Le port du Service
      targetPort: 80    # Le port du Pod
  type: ClusterIP       # IP interne uniquement
```

Puis on d√©ploie le service et on v√©rifie qu‚Äôil est cr√©√© :
![alt text](assets/exercise08.png)

2. Test de la connectivit√© interne

On cr√©e un pod temporaire pour tester l‚Äôacc√®s au service :
![alt text](assets/exercise08-1.png)

On voit bien la page d‚Äôaccueil de NGINX, ce qui confirme que le service ClusterIP fonctionne correctement.

3. V√©rification des endpoints

On liste les endpoints du service pour v√©rifier qu‚Äôils correspondent aux pods du Deployment :
![alt text](assets/exercise08-2.png)

On peut voir que les IPs des pods du Deployment sont bien list√©es comme endpoints du service. Cela confirme que le service ClusterIP est correctement li√© aux pods.

## Exercice 09 - D√©ploiement avec ConfigMap comme volume

Objectif : Apprendre √† utiliser une ConfigMap pour stocker du contenu (ici une page HTML) et le monter dans un pod en tant que volume, afin que NGINX puisse servir ce contenu.

Dans cet exercice, on va cr√©er une ConfigMap contenant une page HTML personnalis√©e, puis on va cr√©er un Deployment NGINX qui monte cette ConfigMap en tant que volume. Ainsi, NGINX servira la page HTML d√©finie dans la ConfigMap.

1. Cr√©ation de la ConfigMap

On cr√©e un fichier index.html avec le contenu suivant :
```html
<h1>Bienvenue sur mon NGINX version ConfigMap !</h1>
<p>Ce contenu est servi directement depuis une ConfigMap.</p>
```

Puis on cr√©e la ConfigMap √† partir de ce fichier :
![alt text](assets/exercise09.png)

2. Cr√©ation du Deployment avec montage de la ConfigMap

On cr√©e un fichier web-deployment-configmap.yaml avec le contenu suivant :
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-logiciel
  template:
    metadata:
      labels:
        app: nginx-logiciel
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
        - name: html-storage
          mountPath: /usr/share/nginx/html/  # Nginx lira notre ConfigMap ici
      volumes:
      - name: html-storage
        configMap:
          name: web-html-config # Le nom de la ConfigMap cr√©√©e √† l'√©tape A
```

Puis on applique le d√©ploiement et on v√©rifie que les pods sont cr√©√©s et que le deployment est accessible:
![alt text](assets/exercise09-1.png)

3. Mise √† jour de la ConfigMap

On modifie le fichier index.html pour changer le message de bienvenue, on met √† jour la ConfigMap, puis on red√©marre les pods pour qu‚Äôils prennent en compte la nouvelle version de la ConfigMap.:
![alt text](assets/exercise09-2.png)

Enfin, on v√©rifie que la nouvelle page est bien servie par NGINX :
![alt text](assets/exercise09-3.png)

On peut voir que NGINX sert bien le contenu mis √† jour depuis la ConfigMap, d√©montrant ainsi comment utiliser une ConfigMap comme volume dans un pod Kubernetes.

## Exercice 10 - D√©ployer MariaDB avec volume persistant

Objectif : Apprendre √† d√©ployer une base de donn√©es MariaDB sur Kubernetes en utilisant un PersistentVolume (PV) et un PersistentVolumeClaim (PVC) pour le stockage persistant des donn√©es.

Dans cet exercice, on va cr√©er un PV utilisant un hostPath pour le stockage local sur le n≈ìud, un PVC pour r√©clamer ce stockage, puis d√©ployer MariaDB en utilisant ce PVC pour stocker ses donn√©es. L'objectif est de s'assurer que les donn√©es de la base persistent m√™me si le pod MariaDB est red√©marr√© ou recr√©√©.

1. Cr√©ation du secret pour les identifiants MariaDB

MariaDB n√©cessite un mot de passe pour l'utilisateur root. On va cr√©er un secret Kubernetes pour stocker ce mot de passe de mani√®re s√©curis√©e.
![alt text](assets/exercise10.png)

2. Cr√©ation du PersistentVolume (PV)

Le PV d√©finit la capacit√© de stockage et le chemin sur le n≈ìud o√π les donn√©es seront stock√©es. On cr√©e un fichier mariadb-pv.yaml avec le contenu suivant :
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mariadb-pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data-mariadb"
```

Puis on applique le PV et on v√©rifie qu‚Äôil est cr√©√© :
![alt text](assets/exercise10-1.png)

Avec ce PV, les donn√©es de MariaDB seront stock√©es localement sur le n≈ìud dans le r√©pertoire `/mnt/data-mariadb`. Ce type de volume est p√©dagogique, mais pas recommand√© en production car il ne supporte pas la haute disponibilit√© ni la portabilit√© des donn√©es entre n≈ìuds. De plus, comme le deployment force l'ex√©cution sur k8s-worker1, le r√©pertoire `/mnt/data-mariadb` doit √™tre cr√©√© manuellement sur ce n≈ìud avant le d√©ploiement.
![alt text](assets/exercise10-2.png)

3. Cr√©ation du PersistentVolumeClaim (PVC)

Le PVC permet de r√©clamer une partie du stockage d√©fini par le PV. On cr√©e un fichier mariadb-pvc.yaml avec le contenu suivant :
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mariadb-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
```

Puis on applique le PVC et on v√©rifie qu‚Äôil est cr√©√© :
![alt text](assets/exercise10-3.png)

4. D√©ploiement de MariaDB

Maintenant qu‚Äôon a le PV et le PVC, on peut d√©ployer MariaDB en utilisant le PVC pour le stockage des donn√©es. On cr√©e un fichier mariadb-deploy.yaml avec le contenu suivant :
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mariadb
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mariadb
  template:
    metadata:
      labels:
        app: mariadb
    spec:
      nodeSelector:
        kubernetes.io/hostname: k8s-worker1
      containers:
      - name: mariadb
        image: mariadb:10.6
        env:
        - name: MARIADB_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mariadb-pass
              key: password
        - name: MARIADB_DATABASE
          value: mabase
        ports:
        - containerPort: 3306
        volumeMounts:
        - name: storage
          mountPath: /var/lib/mysql
      volumes:
      - name: storage
        persistentVolumeClaim:
          claimName: mariadb-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: mariadb-service
spec:
  selector:
    app: mariadb
  ports:
    - port: 3306
      targetPort: 3306
```

Puis on applique le d√©ploiement et on v√©rifie que le pod MariaDB est cr√©√© et en cours d‚Äôex√©cution :
![alt text](assets/exercise10-4.png)

5. Test de connectivit√© √† MariaDB

On cr√©e un pod temporaire pour tester la connexion √† MariaDB :
![alt text](assets/exercise10-5.png)

6. V√©rification de la persistance des donn√©es

Enfin, on va v√©rifier que les donn√©es persistent m√™me apr√®s le red√©marrage du pod MariaDB.

On commence par cr√©er une base de donn√©es et une table et on ins√®re des donn√©es :
![alt text](assets/exercise10-6.png)

Ensuite, on supprime le pod MariaDB pour simuler un red√©marrage et on v√©rifie que le pod est recr√©√© automatiquement par le deployment :
![alt text](assets/exercise10-7.png)

Apr√®s le red√©marrage, on se reconnecte √† MariaDB et on v√©rifie que les donn√©es ins√©r√©es pr√©c√©demment sont toujours pr√©sentes :
![alt text](assets/exercise10-8.png)

On peut voir que les donn√©es ins√©r√©es avant le red√©marrage du pod sont toujours pr√©sentes, ce qui confirme que le stockage persistant via le PersistentVolume et le PersistentVolumeClaim fonctionne correctement avec MariaDB sur Kubernetes.

## Exercice 11 - D√©ployer une application PHP connect√©e √† MariaDB

Objectif : D√©ployer une application PHP simple sur Kubernetes qui se connecte √† une base de donn√©es MariaDB, en utilisant des ConfigMaps et des Secrets pour la configuration.

Dans cet exercice, on va cr√©er une ConfigMap pour le code PHP, un Secret pour le mot de passe de la base de donn√©es, puis d√©ployer une application PHP qui se connecte √† MariaDB.

On suppose que MariaDB est d√©j√† d√©ploy√©e et accessible via le service `mariadb-service`.

1. Cr√©ation de la ConfigMap pour le code PHP

On cr√©e un fichier index.php avec le contenu suivant :
```php
apiVersion: v1
kind: ConfigMap
metadata:
  name: php-code
data:
  index.php: |
    <?php
    $host = 'mariadb-service';
    $db   = 'mabase';
    $user = 'root';
    $pass = getenv('DB_PASSWORD'); // üîê Mot de passe depuis le Secret

    try {
        $dsn = "mysql:host=$host;dbname=$db;charset=utf8mb4";
        $pdo = new PDO($dsn, $user, $pass);

        echo "<body style='font-family:sans-serif; text-align:center; padding-top:50px; background-color:#f0fff4;'>";
        echo "<h1 style='color:#2f855a;'>‚úÖ Connexion R√©ussie !</h1>";
        echo "<p>PHP communique correctement avec MariaDB.</p>";
        echo "<p><b>Service DB :</b> $host</p>";
        echo "<p><b>IP Pod PHP :</b> " . $_SERVER['SERVER_ADDR'] . "</p>";
        echo "</body>";
    } catch (PDOException $e) {
        echo "<body style='font-family:sans-serif; text-align:center; padding-top:50px; background-color:#fff5f5;'>";
        echo "<h1 style='color:#c53030;'>‚ùå Erreur de Connexion</h1>";
        echo "<p>" . $e->getMessage() . "</p>";
        echo "</body>";
    }
    ?>
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: php-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: php-web
  template:
    metadata:
      labels:
        app: php-web
    spec:
      containers:
      - name: php
        image: php:8.0-apache
        command: ["sh", "-c", "docker-php-ext-install pdo pdo_mysql && apache2-foreground"]
        ports:
        - containerPort: 80
        env:
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mariadb-pass
              key: password
        volumeMounts:
        - name: code-volume
          mountPath: /var/www/html/index.php
          subPath: index.php
      volumes:
      - name: code-volume
        configMap:
          name: php-code
---
apiVersion: v1
kind: Service
metadata:
  name: php-service
spec:
  type: ClusterIP
  selector:
    app: php-web
  ports:
    - port: 80
      targetPort: 80
```

Puis on applique la ConfigMap :
![alt text](assets/exercise11.png)

2. V√©rification de la ConfigMap

Et on v√©rifie qu‚Äôelle est cr√©√©e :
![alt text](assets/exercise11-1.png)

On voit sur le screenshot ci-dessus que la ConfigMap contient bien le code PHP.

### Activit√© 1: Compr√©hension de l'architecture

A partir des manifestes fournis, on va analyser l'architecture de l'application PHP connect√©e √† MariaDB, et identifier les diff√©rents composants et leur r√¥le. On r√©alisera √©galement un sch√©ma simple de l'architecture.


On utilise quelques commandes basiques pour explorer les ressources cr√©√©es :
![alt text](assets/exercise11-2.png)

On en d√©duit:
- **Le pod MariaDB :** `mariadb-6bdb6b75c-6qfc2` (IP: `10.244.3.25`, **Node :** `k8s-worker1`)
- **Les pods PHP :** `php-app-569f46cdf7-hz84s` (IP: `10.244.1.22`, **node** `k8s-worker3`) et `php-app-569f46cdf7-tfps4` (IP: `10.244.2.38`, **node** `k8s-worker2`)
- **Les services :**
  - `mariadb-service` - **ClusterIP** `10.101.80.222` (port 3306) ‚Üí point d‚Äôacc√®s DB interne.
  - `php-service` - **ClusterIP** `10.105.190.224` (port 80) ‚Üí service interne pour l‚Äôapp PHP.
  - `nginx` - **NodePort** `10.102.91.113` (80:31506) ‚Üí expos√© sur les n≈ìuds.
  - `web-deployment` - **NodePort** `10.107.20.178` (80:32176).
  - `web-service-unique` - **ClusterIP** `10.111.171.64` (port 80).
- **Les volumes utilis√©s :**
  - PersistentVolume **`mariadb-pv`** (1Go) - **li√©** √† PersistentVolumeClaim **`mariadb-pvc`**. Ce stockage permet la persistance des donn√©es MariaDB.
- **Les Secrets & ConfigMaps :**
  - **Secrets :** `mariadb-pass` (contient la cl√© `password` utilis√©e par MariaDB), `db-secret`, `regcred` (docker registry cred).
  - **ConfigMaps :** `php-code` (contient `index.php` mont√© dans les pods PHP), `nginx-config`, `web-html-config`, `kube-root-ca.crt`.

Diff√©rence entre Service et Pod:
- Un **Pod** est l'unit√© de base de d√©ploiement dans Kubernetes, repr√©sentant un ou plusieurs conteneurs qui partagent le m√™me r√©seau et stockage.
- Un **Service** est une abstraction qui d√©finit un ensemble logique de pods et une politique d'acc√®s √† ces pods, permettant la d√©couverte et la communication entre eux.

Diff√©rence entre ConfigMap et Secret:
- Une **ConfigMap** est utilis√©e pour stocker des donn√©es de configuration non sensibles sous forme de paires cl√©-valeur.
- Un **Secret** est utilis√© pour stocker des informations sensibles, telles que des mots de passe ou des cl√©s API, de mani√®re s√©curis√©e.

### Activit√© 2: V√©rification de la connexion r√©seau

Depuis un Pod PHP (`php-app`), on v√©rifie la r√©solution DNS et la connectivit√© vers le service `mariadb-service` : le nom se r√©sout correctement et la connexion MySQL est fonctionnelle.

On utilise `mariadb-service` plut√¥t qu'une IP parce que le Service fournit un point d'acc√®s stable (DNS + IP virtuelle) et effectue du load‚Äëbalancing vers les pods backend ; les IP des pods sont √©ph√©m√®res et peuvent changer lors des red√©ploiements ou d√©placements des pods.

### Activit√© 3: Cr√©ation manuelle de donn√©es

On cr√©e une table posts et on ajoute quelques entr√©es manuellement via le pod mariadb.

![alt text](assets/exercise11-3.png)

### Activit√© 4: Test de la persistance des donn√©es

On va supprimer le pod MariaDB pour simuler un red√©marrage, puis v√©rifier que les donn√©es ins√©r√©es pr√©c√©demment sont toujours pr√©sentes.

![alt text](assets/exercise11-4.png)

On voit sur le screenshot que les donn√©es ins√©r√©es avant le red√©marrage du pod sont toujours pr√©sentes, ce qui confirme que le stockage persistant via le PersistentVolume et le PersistentVolumeClaim fonctionne correctement avec MariaDB sur Kubernetes.

Le volume persistant permet de conserver les donn√©es m√™me si le pod est supprim√© ou red√©marr√©, assurant ainsi la durabilit√© des donn√©es de la base MariaDB.

### Activit√© 5: Modifier le code PHP via ConfigMap

Nombre d'utilisateurs dans la table:
![alt text](assets/exercise11-5.png)

On met maintenant √† jour la ConfigMap `php-app` pour modifier le code PHP.

On applique la nouvelle ConfigMap et on red√©marre les pods PHP pour qu‚Äôils prennent en compte la nouvelle version du code :

![alt text](assets/exercise11-6.png)

### Activit√© 6: Simulation de panne applicative

On va simuler une panne de l‚Äôapplication PHP en supprimant un pod PHP, puis v√©rifier que Kubernetes recr√©e automatiquement un nouveau pod pour maintenir le nombre de r√©plicas d√©fini dans le Deployment.

![alt text](assets/exercise11-7.png)

Les r√©plicas sont bien maintenus √† 2, et le nouveau pod recr√©√© fonctionne correctement.

Les r√©plicas assurent la haute disponibilit√© de l‚Äôapplication en maintenant un nombre constant de pods en cours d‚Äôex√©cution. Si un pod √©choue ou est supprim√©, Kubernetes cr√©e automatiquement un nouveau pod pour remplacer celui qui a √©t√© perdu, garantissant ainsi que l‚Äôapplication reste disponible et fonctionnelle.

###¬†Activit√© 7: D√©bogage kubernetes

### Activit√© 7 ‚Äì D√©bogage Kubernetes (bonus ‚≠ê)

Dans cette activit√©, on va volontairement introduire des erreurs dans la configuration de l‚Äôapplication PHP connect√©e √† MariaDB, puis utiliser les outils de diagnostic Kubernetes pour identifier et comprendre les probl√®mes.

#### 1. Erreur : mauvais nom de service

On modifie la ConfigMap contenant le code PHP pour utiliser un nom de service inexistant (par exemple `mariadb-service-faux` au lieu de `mariadb-service`). Apr√®s avoir red√©marr√© les pods PHP, l‚Äôapplication ne parvient plus √† se connecter √† la base de donn√©es.

Pour diagnostiquer l‚Äôerreur :
```bash
kubectl logs <nom-du-pod-php>
```
On observe un message d‚Äôerreur indiquant l‚Äôimpossibilit√© de r√©soudre le nom DNS ou d‚Äô√©tablir la connexion √† la base.

On peut √©galement utiliser :
```bash
kubectl describe pod <nom-du-pod-php>
```
pour v√©rifier les √©v√©nements r√©cents et les √©ventuelles erreurs de d√©marrage.

#### 2. Erreur : mauvais mot de passe DB

On modifie le Secret `mariadb-pass` pour y mettre un mot de passe incorrect, puis on red√©marre les pods PHP. L‚Äôapplication √©choue √† se connecter √† MariaDB.

Diagnostic :
```bash
kubectl logs <nom-du-pod-php>
```
affiche une erreur d‚Äôauthentification MySQL (ex : `Access denied for user 'root'@'...'`).

#### Questions de r√©flexion

- **Pourquoi ne pas mettre le mot de passe DB dans le code PHP ?**
  
  Mettre le mot de passe en dur dans le code PHP expose un risque de s√©curit√© : toute personne ayant acc√®s au code source peut lire le mot de passe. Utiliser un Secret Kubernetes permet de s√©parer les informations sensibles du code et de limiter leur exposition.

- **Quelle diff√©rence entre red√©marrer un Pod et red√©ployer une application ?**
  
  Red√©marrer un Pod (par exemple en le supprimant) entra√Æne la recr√©ation d‚Äôun pod identique par le Deployment, sans modification de la configuration ou de l‚Äôimage. Red√©ployer une application (par exemple apr√®s modification du manifest ou de l‚Äôimage) met √† jour la configuration, l‚Äôimage ou les variables d‚Äôenvironnement, et peut entra√Æner la cr√©ation de nouveaux pods avec ces changements.

- **Que se passe-t-il si le n≈ìud Kubernetes tombe ?**
  
  Si un n≈ìud tombe, tous les pods qui y √©taient h√©berg√©s deviennent indisponibles. Kubernetes d√©tecte l‚Äôindisponibilit√© du n≈ìud et recr√©e automatiquement les pods sur les autres n≈ìuds disponibles (si les ressources le permettent), assurant ainsi la haute disponibilit√© de l‚Äôapplication.

## Exercice 12 - Red√©ploiement et debug applicatif

Objectif : Comprendre le lien entre ConfigMap et Pods, et identifier pourquoi une modification de ConfigMap ne provoque pas automatiquement un red√©ploiement.

Dans cet exercice, on va modifier une ConfigMap utilis√©e par un pod, puis observer que le pod ne prend pas en compte la modification tant qu‚Äôil n‚Äôest pas red√©marr√©. On expliquera pourquoi et comment forcer la prise en compte des modifications.

On commence par cr√©er le fichier nginx-configmap.yaml avec le contenu suivant :

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: new-php-code
data:
  index.php: |
    <?php
    $host = 'mariadb-service'; // On utilise le nom DNS du service cr√©√© √† l'exo pr√©c√©dent
    $db   = 'mabase';
    $user = 'root';
    $pass = getenv('DB_PASSWORD');;   // Le mot de passe d√©fini dans ton Secret

    try {
        $dsn = "mysql:host=$host;dbname=$db;charset=utf8mb4";
        $pdo = new PDO($dsn, $user, $pass);

        echo "<body style='font-family:sans-serif; text-align:center; padding-top:50px; background-color:#f0fff4;'>";
        echo "<h1 style='color:#2f855a;'>‚úÖ Connexion R√©ussie !</h1>";
        echo "<p>L'application PHP communique bien avec MariaDB sur le port 3306.</p>";
        echo "<div style='border:1px solid #ccc; display:inline-block; padding:20px; border-radius:10px; background:white;'>";
        echo "<b>Infos Cluster :</b><br>";
        echo "Serveur DB : " . $host . "<br>";
        echo "IP du Pod PHP : " . $_SERVER['SERVER_ADDR'];

        $stmt = $pdo->query("SELECT contenu FROM posts");
        while ($row = $stmt->fetch()) {
         echo "<p>Contenu trouv√© en base : <b>" . $row['contenu'] . "</b></p>";
        }
    } catch (PDOException $e) {
        echo "<body style='font-family:sans-serif; text-align:center; padding-top:50px; background-color:#fff5f5;'>";
        echo "<h1 style='color:#c53030;'>‚ùå Erreur de Connexion</h1>";
        echo "<p>Message : " . $e->getMessage() . "</p>";
        echo "</body>";
    }
    echo "</div></body>";
    ?>
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: new-php-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: new-php-web
  template:
    metadata:
      labels:
        app: new-php-web
    spec:
      containers:
      - name: php
        image: php:8.0-apache
        # On installe l'extension PDO MySQL au d√©marrage (astuce pour image de base)
        command: ["sh", "-c", "docker-php-ext-install pdo pdo_mysql && apache2-foreground"]
        ports:
        - containerPort: 80
        volumeMounts:
        - name: code-volume
          mountPath: /var/www/html/index.php
          subPath: index.php
      volumes:
      - name: code-volume
        configMap:
          name: new-php-code
---
apiVersion: v1
kind: Service
metadata:
  name: new-php-service
spec:
  type: ClusterIP
  selector:
    app: new-php-web
  ports:
    - port: 80
      targetPort: 80
```

Puis on le d√©ploie et on v√©rifie le d√©ploiement avec les commandes suivantes:
```bash
kubectl apply -f php-app.yaml

kubectl get pods
kubectl get svc

kubectl describe pod -l app=new-php-web
kubectl describe svc new-php-service
```

On va maintenant ex√©cuter la commande fournie dans le sujet:
```bash
kubectl run curlpod --rm -it --image=curlimages/curl --restart=Never -- curl http://new-php-service
```

On s'attend √† voir le message de bienvenue d√©fini dans la ConfigMap. Cependant, on constate √† la place qu'on obtient une erreur de connexion √† la base de donn√©es.

![alt text](assets/exercise12.png)

On remarque dans l'erreur, que la connexion s'est faite sans mot de passe (using password: NO), ce qui indique que la variable d'environnement `DB_PASSWORD` n'est pas inject√©e dans le pod.

En examinant le code PHP dans la ConfigMap, on voit que la variable d'environnement `DB_PASSWORD` est utilis√©e pour r√©cup√©rer le mot de passe de la base de donn√©es :
```php
$pass = getenv('DB_PASSWORD');;
```

Ce mot de passe est cens√© provenir d'un Secret Kubernetes, dont on peut d'ailleurs v√©rifier l'existence avec la commande:
```bash
kubectl get secret mariadb-pass -o yaml
```
![alt text](assets/exercise12-1.png)

V√©rifions que le pod `new-php-app` a bien acc√®s √† ce Secret en listant les variables d'environnement du pod:
![alt text](assets/exercise12-2.png)

On constate que la variable d'environnement `DB_PASSWORD` n'est pas d√©finie dans le pod `new-php-app`. C'est la raison pour laquelle la connexion √† MariaDB √©choue.

En comparant avec le pod de l'exercice pr√©c√©dent (`php-app`), on remarque que ce dernier a bien la variable `DB_PASSWORD` d√©finie, car dans son manifeste, le pod utilise un Secret pour injecter cette variable d'environnement.

Pour r√©soudre ce probl√®me, on peut donc modifier le manifeste pour inclure la d√©finition de la variable d'environnement `DB_PASSWORD` √† partir du Secret `mariadb-pass`, comme ceci:
```yaml
        env:
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mariadb-pass
              key: password
```

En v√©rifiant l'√©tat de la base de donn√©e, on constate aussi un autre probl√®me: la table `posts` ne contient pas de colonne `contenu`, mais une colonne `content`. On corrige donc la requ√™te SQL dans le code PHP pour refl√©ter cette modification:
```php
$stmt = $pdo->query("SELECT content FROM posts");
```

Apr√®s avoir apport√© ces modifications, on red√©ploie le manifeste corrig√© et on red√©marre les pods pour qu‚Äôils prennent en compte les changements:
![alt text](assets/exercise12-3.png)

Enfin, on ex√©cute √† nouveau la commande curl pour tester l‚Äôapplication:
![alt text](assets/exercise12-4.png)

Tout fonctionne ! On voit que l‚Äôapplication PHP se connecte correctement √† MariaDB et affiche les donn√©es de la table `posts`.

## Exercice 13 - Acc√®s via Ingress

Objectif : Comprendre le r√¥le d'un Ingress dans Kubernetes pour exposer des services HTTP/S externes, et diagnostiquer les probl√®mes de routage.

On commence par cr√©er un fichier new-app-ingress.yaml avec le contenu suivant :
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: new-php-app-ingress
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  ingressClassName: nginx
  rules:
  - host: example.lab
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: new-php-service
            port:
              number: 80
```

Puis on applique le manifeste et on v√©rifie que l‚ÄôIngress est cr√©√© et qu'on peut y acc√©der via curl :
![alt text](assets/exercise13.png)

On constate que la requ√™te curl vers `http://example.lab` √©choue avec une erreur de connexion refus√©e.

Si on teste sans Ingress, en mettant en place un port-forwarding vers le service `new-php-service`, on peut acc√©der √† l‚Äôapplication PHP sans probl√®me :
![alt text](assets/exercise13-1.png)

Cela indique que le service `new-php-service` fonctionne correctement, mais que l‚ÄôIngress ne parvient pas √† router le trafic vers ce service.

On v√©rifie la configuration de l‚ÄôIngress et on remarque que le nom d‚Äôh√¥te `example.lab` n‚Äôest pas r√©solu correctement. Pour que l‚ÄôIngress fonctionne, il faut que le nom d‚Äôh√¥te soit mapp√© √† l‚Äôadresse IP du contr√¥leur Ingress (NGINX).

On ajoute une entr√©e dans le fichier `/etc/hosts` de notre machine locale pour faire pointer `example.lab` vers l‚ÄôIP du n≈ìud (r√©cup√©r√©e via `kubectl get nodes -o wide`).

Ensuite, on effectue √† nouveau une requ√™te HTTP vers `example.lab`. On peut maintenant voir que la requ√™te est maintenant correctement rout√©e vers le service PHP via Ingress, et l‚Äôapplication fonctionne comme pr√©vu.

## Exercice 14 - D√©ployer une application Flask + Postgres avec ArgoCD

Objectif : Construire et publier une image Docker, d√©ployer une application Flask connect√©e √† une base de donn√©es Postgres sur Kubernetes, et g√©rer le d√©ploiement avec ArgoCD pour le GitOps.

### 1. Mise en place

On clone le repo fourni et on configure GitHub Actions avec les secrets n√©cessaires pour pousser l‚Äôimage Docker vers Docker Hub: nom d'utilisateur et token d'acc√®s.

### 2. Installation d‚ÄôArgoCD sur le cluster Kubernetes

On installe ArgoCD en appliquant les manifests officiels :
```bash
kubectl create namespace argocd

kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
```

On v√©rifie ensuite que tous les Pods sont bien pr√™ts:
![alt text](assets/exercise13-2.png)

On ex√©cute ensuite ce script pour configurer la r√©solution DNS dans les pods ArgoCD, afin de s'assurer qu'ils peuvent r√©soudre les noms de services internes du cluster Kubernetes :
```bash
# 1. Mise √† jour de l'application-controller (StatefulSet)
kubectl patch statefulset argocd-application-controller -n argocd --type strategic -p '
spec:
  template:
    spec:
      dnsPolicy: "None"
      dnsConfig:
        nameservers:
          - 10.96.0.10
          - 8.8.8.8
          - 8.8.4.4
        searches:
          - argocd.svc.cluster.local
          - svc.cluster.local
          - cluster.local
        options:
          - name: ndots
            value: "2"
'

# 2. Mise √† jour du repo-server (Deployment)
kubectl patch deployment argocd-repo-server -n argocd --type strategic -p '
spec:
  template:
    spec:
      dnsPolicy: "None"
      dnsConfig:
        nameservers:
          - 10.96.0.10
          - 8.8.8.8
          - 8.8.4.4
        searches:
          - argocd.svc.cluster.local
          - svc.cluster.local
          - cluster.local
        options:
          - name: ndots
            value: "2"
'

# 3. Red√©marrage des composants pour appliquer les changements
kubectl rollout restart statefulset argocd-application-controller -n argocd
kubectl rollout restart deployment argocd-repo-server -n argocd

# 4. V√©rification du statut du d√©ploiement
kubectl rollout status statefulset argocd-application-controller -n argocd
kubectl rollout status deployment argocd-repo-server -n argocd

# [!IMPORTANT] V√©rifiez que l'IP 10.96.0.10 correspond bien √† l'IP du service kube-dns ou coredns dans votre cluster:
(kubectl get svc -n kube-system).
```

![alt text](assets/exercise14.png)

Ensuite, on r√©cup√®re et on note le mot de passe initial pour se connecter √† l‚Äôinterface web d‚ÄôArgoCD :
```bash
kubectl get secret argocd-initial-admin-secret -n argocd -o jsonpath="{.data.password}" | base64 -d
```
-> `E81l4MGU1mjFPDNt`

Enfin on v√©rifie que le pod ArgoCD est capable de r√©soudre le nom de domaine github.com :
![alt text](assets/exercise14-1.png)

### 3. D√©ploiement la base de donn√©e Postgres

Avant de cr√©er le fichier de d√©ploiement pour la base de donn√©es Postgres, on cr√©e :

- Un **Secret** pour stocker le mot de passe de la base de donn√©es (ex : `postgres-secret`). Exemple :
```bash
kubectl create secret generic postgres-secret --from-literal=POSTGRES_PASSWORD='SuperSecret'
```

- Un **ConfigMap** pour la configuration de la base de donn√©es Postgres (ex : `postgres-config`). Exemple :
```bash
kubectl create configmap postgres-config --from-literal=POSTGRES_DB=mabase --from-literal=POSTGRES_USER=user
```

> Remarque : dans l'exemple ci‚Äëdessus `POSTGRES_DB` et `POSTGRES_USER` sont utilis√©s par le d√©ploiement Postgres et doivent correspondre aux variables dans le manifest.

Il faut aussi cr√©er le PersistentVolume (PV) pour le stockage persistant des donn√©es. On cr√©e un fichier `postgres-pv.yaml` avec le contenu suivant :
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: postgres-pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: ""   # PV "statique" sans StorageClass
  hostPath:
    path: "/mnt/data-postgres"
```

Important : avant d'appliquer ce PV, il faut bien penser √† cr√©er le r√©pertoire sur le noeud cibl√© et d√©finir les permissions :
```bash
sudo mkdir -p /mnt/data-postgres
sudo chown 999:999 /mnt/data-postgres
```

Puis on applique la configuration:
![alt text](assets/exercise14-5.png)

On peut ensuite cr√©er le manifest pour la configuration de la base de donn√©es Postgres et la d√©ployer (PVC, Deployment, Service) :
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
  labels:
    app: postgres
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres
  labels:
    app: postgres
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
        - name: postgres
          image: postgres:15
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 5432
              name: postgres
          env:
            - name: POSTGRES_DB
              valueFrom:
                configMapKeyRef:
                  name: postgres-config
                  key: POSTGRES_DB
                  optional: true
            - name: POSTGRES_USER
              valueFrom:
                configMapKeyRef:
                  name: postgres-config
                  key: POSTGRES_USER
                  optional: true
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: POSTGRES_PASSWORD
          volumeMounts:
            - name: data
              mountPath: /var/lib/postgresql/data
          readinessProbe:
            tcpSocket:
              port: 5432
            initialDelaySeconds: 10
            periodSeconds: 5
          livenessProbe:
            tcpSocket:
              port: 5432
            initialDelaySeconds: 30
            periodSeconds: 10
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: postgres-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: postgres-service
  labels:
    app: postgres
spec:
  type: ClusterIP
  ports:
    - port: 5432
      targetPort: 5432
      protocol: TCP
      name: postgres
  selector:
    app: postgres
```
![alt text](assets/exercise14-4.png)

V√©rification : une fois appliqu√©s, on v√©rifie bien que le PV et le PVC sont **Bound** :
![alt text](assets/exercise14-9.png)

On v√©rifie ensuite que le pod Postgres est en cours d‚Äôex√©cution et que le service est accessible :
![alt text](assets/exercise14-6.png)

### 4. Cr√©ation de l'image Docker de l'application Flask

On va maintenant cr√©er l‚Äôimage Docker pour l‚Äôapplication Flask.

On dispose d√©j√† d'un fichier Dockerfile minimal dans le r√©pertoire `app`. On peut donc directement construire l‚Äôimage Docker en utilisant la commande suivante :
![alt text](assets/exercise14-7.png)

On pousse ensuite l‚Äôimage vers Docker Hub :
```bash
docker login
docker push simonleclere/flask-postgres-app:latest
```

![alt text](assets/exercise14-8.png)

On a maintenant l‚Äôimage Docker de l‚Äôapplication Flask disponible sur Docker Hub.

### 5. D√©ploiement de l'application Flask sur Kubernetes

On cr√©e un fichier flask-app.yaml avec le contenu suivant :
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flask-app
spec:
  replicas: 4
  selector:
    matchLabels:
      app: flask
  template:
    metadata:
      labels:
        app: flask
    spec:
      hostAliases:
      - ip: "140.82.121.3"
        hostnames: ["github.com"]
      containers:
      - name: flask-app
        image: simonleclere/flask-postgres-app:latest
        imagePullPolicy: Always
        env:
        - name: DB_HOST
          value: "postgres-service"
        - name: DB_NAME
          valueFrom: { configMapKeyRef: { name: db-config, key: POSTGRES_DB } }
        - name: DB_USER
          valueFrom: { configMapKeyRef: { name: db-config, key: POSTGRES_USER } }
        - name: DB_PASSWORD
          valueFrom: { secretKeyRef: { name: db-secret, key: POSTGRES_PASSWORD } }
        ports:
        - containerPort: 5000
---
apiVersion: v1
kind: Service
metadata:
  name: flask-service
spec:
  selector:
    app: flask
  ports:
  - port: 80
    targetPort: 5000
```

Puis on applique le manifeste et on v√©rifie que le pod Flask est cr√©√© et en cours d‚Äôex√©cution :
![alt text](assets/exercise14-10.png)

### 6. D√©ploiemnt de l'Ingress

On va d√©ployer un Ingress pour exposer l‚Äôapplication Flask. On cr√©e un fichier ingress.yaml avec le contenu suivant :
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: flask-app-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: flask.lab
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: flask-service
            port:
              number: 80
```

Puis on applique le manifeste puis on v√©rifie que l‚ÄôIngress est cr√©√© :
![alt text](assets/exercise14-11.png)


Pour simplifier l'acc√®s, on peut identifier l‚ÄôIP du noeud Kubernetes pour acc√©der √† l‚ÄôIngress et l'ajouter dans le fichier `/etc/hosts` pour le nom `flask.lab`.

Enfin, on peut curl l'application ou essayer d'y acc√©der pour v√©rifier que tout fonctionne :
![alt text](assets/exercise14-12.png)

On voit que l‚Äôapplication Flask se connecte correctement √† la base de donn√©es Postgres et affiche le nombre de visites stock√©es en base.

### 7. Mise √† jour du d√©ploiement avec ArgoCD

Directement sur l'interface github, on modifie le fichier `flask-app.yaml` pour augmenter le nombre de r√©plicas de 4 √† 6 :
```yaml
spec:
  replicas: 6
```

Une fois la synchronisation effectu√©e par ArgoCD, on v√©rifie que le nombre de pods Flask a bien √©t√© mis √† jour :
![alt text](assets/exercise14-13.png)