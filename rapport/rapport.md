---
Simon Leclere
2026
---

# TP terraform & kubernetes - Partie 1

Dans cette premi√®re partie, on va utiliser les scripts et la configuration fournie pour d√©ployer un cluster Kubernetes sur Proxmox via Terraform et Ansible.

Rappel des termes :
- **Proxmox VE** : hyperviseur de virtualisation utilis√© pour h√©berger les machines virtuelles.
- **Template** : image VM pr√©configur√©e utilis√©e pour cloner des VMs identiques.
- **Terraform** : outil d'IaC utilis√© pour cr√©er les VMs et les ressources dans Proxmox via un provider.
- **Ansible** : outil d'orchestration pour configurer les nodes (installation de containerd, kubeadm, kubelet, kubectl, etc.).
- **Kubernetes**: orchestrateur de conteneurs pour d√©ployer et g√©rer des applications conteneuris√©es.
- **kubeadm / kubelet / kubectl** : composants Kubernetes pour initialiser et g√©rer le cluster.

## Etape 1: Cr√©ation du template de VM

Avant de commencer le TP, on a d√©j√† une installation proxmox fonctionnelle, avec un acc√®s root via SSH configur√©, un stockage NFS mont√© et deux bridges vmbr0 et vmbr1.

Dans un premier temps, on commence par pr√©parer l'image de base, qui sera plus tard clon√©e par terraform pour cr√©er les machines. Cela permet de partir d'une image propre et reproductible, avec les paquets et l'agent cloud-init correctement configur√©s.

Pour √ßa, on ex√©cute le script `k8s-proxmox-iac/scripts/create_vm_template.sh`, qui pr√©pare et convertit une VM Ubuntu Noble 24.04 LTS en template pour Proxmox VE (installation des paquets utiles, configuration SSH, nettoyage des logs et des identifiants uniques). Le template sert ensuite de source pour cloner les machines lors du d√©ploiement Terraform.

![Sortie du script de cr√©ation de template](assets/script_create_vm_template.png)

## Etape 2: D√©ploiement Terraform

La configuration Terraform se trouve dans `k8s-proxmox-iac/terraform` :
- `providers.tf` contient la configuration du provider Proxmox,
- `variables.tf` et `terraform.tfvars` d√©finissent le nombre de nodes, ressources et adresses r√©seau,
- un module `modules/k8s-node` permet de factoriser la cr√©ation des VMs.

Avant de le provisionning terraform, quelques changements sont quand m√™me n√©cessaires. Notamment dans `terraform.tfvars` pour adapter les param√®tres r√©seau √† notre environnement Proxmox. Aussi, on change de version pour le provider Proxmox dans `providers.tf` pour √©viter des probl√®mes de compatibilit√©.

On applique la configuration avec les commandes suivantes:
```bash
terraform init # initialise le backend et t√©l√©charge les plugins
terraform plan # affiche le plan d'ex√©cution
terraform apply # applique la configuration et cr√©e les ressources
```

![alt text](assets/tfapply.png)

Apr√®s `apply`, Terraform retourne des outputs (IP des machines, IDs, etc.) utilisables pour l'inventaire Ansible.

On peut √©galement voir les VMs cr√©√©es dans l'interface Proxmox :
![Etat de proxmox apr√®s le provisionning via Terraform](assets/proxmox.png)

## Etape 3: Configuration via Ansible

Maintenant qu'on a l'infrastructure de base (les VMs) cr√©√©e par Terraform, on utilise Ansible pour configurer les nodes et installer Kubernetes. Pour ca, on dispose de 4 playbooks dans `k8s-proxmox-iac/ansible/playbooks`, qui seront ex√©cut√©s dans l'ordre.

Avant de les ex√©cuter, on s'assure que notre machine locale peut se connecter aux VMs via SSH en utilisant la cl√© priv√©e g√©n√©r√©e par le script de cr√©ation de template (situ√©e dans `k8s-proxmox-iac/scripts/id_rsa_proxmox_templates`). Si besoin, on cr√©e un tunnel SSH pour acc√©der aux VMs si elles ne sont pas directement accessibles.

### `01-prepare-nodes.yml` : pr√©paration des nodes (utilisateurs, SSH, d√©pendances syst√®me)

![alt text](assets/playbook01.png)

### `02-install-k8s.yml` : installation de `containerd`, `kubeadm`, `kubelet`, `kubectl` et verrouillage des versions

![alt text](assets/playbook02.png)

### `03-init-master.yml` : initialisation du master avec `kubeadm init`, configuration de `~/.kube/config`, installation du r√©seau Pod (ici Flannel via `kube-flannel.yml`) et g√©n√©ration de la commande de join qui est enregistr√©e dans `/tmp/k8s_join_command.sh`,

![alt text](assets/playbook03.png)

### `04-join-workers.yml` : ex√©cution de la commande de jonction sur les workers pour les ajouter au cluster.

![alt text](assets/playbook04.png)

## Etape 4: V√©rifications et conclusion

Apr√®s l'ex√©cution des playbooks, on devrait normalement avoir un cluster Kubernetes fonctionnel avec un master et 3 workers. Pour v√©rifier l'√©tat du cluster, on peut se connecter au master et ex√©cuter quelques commandes :
- `kubectl get nodes` -> v√©rifier que tous les nodes apparaissent en `Ready`
![alt text](assets/kubectlnodes.png)

- `kubectl get pods -A` -> v√©rifier que les pods du plan de contr√¥le et du CNI (Flannel) sont en `Running`

![alt text](assets/kubectlpods.png)

Pour la suite du TP, on simplifiera les commandes en copiant le fichier kubeconfig du master vers notre poste local, ce qui nous permettra d'utiliser `kubectl` directement depuis notre machine.

# Partie 2 : Exercices

On dispose d'un dossier `Exercises` contenant plusieurs exercices pratiques pour se familiariser avec Kubernetes : services, volumes persistants, ConfigMaps, d√©ploiement d'une application PHP connect√©e √† MariaDB, etc.

Pour certains, quelques questions de r√©flexion sont pos√©es dans les fichiers README.md.

## Exercice 01 ‚Äî D√©ployer une application de test sur Kubernetes

Objectif : D√©ployer une application simple (`nginx`) sur un cluster Kubernetes afin de v√©rifier le bon fonctionnement du cluster, du r√©seau et des services.

On cr√©e un d√©ploiement NGINX avec 3 r√©plicas, on expose le d√©ploiement via un Service de type NodePort, puis on teste l'acc√®s √† l'application.

```bash
# On cr√©e un d√©ploiement nginx avec 3 replicas
kubectl create deployment nginx --image=nginx --replicas=3

# On v√©rifie que les pods ont bien √©t√© cr√©√©s
kubectl get pods

# On expose le d√©ploiement via un NodePort
kubectl expose deployment nginx --port=80 --type=NodePort

# On r√©cup√®re les informations du service (port NodePort attribu√©)
kubectl get svc nginx

# Enfin, on teste l'acc√®s √† l'application depuis l'ext√©rieur du cluster
curl http://10.0.0.11:31711
```

On peut voir les pods nginx en √©tat Running et le service expos√© en NodePort:
![alt text](assets/exercice01-1.png)

Si on ouvre l'url, on voit bien la page d'accueil NGINX s'afficher:
![alt text](assets/exercise01.png)

### Questions:

- Sur quels n≈ìuds les pods nginx sont-ils d√©ploy√©s ?
> Les pods nginx sont d√©ploy√©s sur les n≈ìuds workers du cluster Kubernetes. On peut v√©rifier cela en utilisant la commande `kubectl get pods -o wide`, qui affiche le n≈ìud sur lequel chaque pod est h√©berg√©.

- Que se passe-t-il si vous supprimez un pod nginx ?
> Si un pod nginx est supprim√©, Kubernetes d√©tecte que le nombre de r√©plicas sp√©cifi√© dans le d√©ploiement n'est plus respect√©. En cons√©quence, il cr√©e automatiquement un nouveau pod pour remplacer celui qui a √©t√© supprim√©, assurant ainsi que le nombre de r√©plicas reste constant √† 3.

- Quelle est la diff√©rence entre un Service ClusterIP et NodePort ?
> Un Service ClusterIP est le type de service par d√©faut dans Kubernetes. Il expose le service uniquement √† l'int√©rieur du cluster, permettant aux pods de communiquer entre eux via une adresse IP interne. En revanche, un Service NodePort expose le service sur un port sp√©cifique de chaque n≈ìud du cluster, permettant l'acc√®s au service depuis l'ext√©rieur du cluster en utilisant l'adresse IP du n≈ìud et le port NodePort attribu√©.

##¬†Exercice 02 ‚Äî Mise √† l‚Äô√©chelle (Scaling) d‚Äôune application

Objectif : Apprendre √† mettre √† l‚Äô√©chelle un d√©ploiement Kubernetes en modifiant le nombre de r√©plicas d‚Äôune application, et observer comment le cluster r√©agit automatiquement.

Contexte : Dans l‚Äôexercice pr√©c√©dent, on a d√©ploy√© un d√©ploiement `nginx` avec 3 r√©plicas.

L‚Äôobjectif maintenant est de :
- Augmenter le nombre de pods pour g√©rer plus de trafic
- R√©duire le nombre de pods si n√©cessaire
- Observer la r√©action du cluster

On voit sur le screenshot ci-dessous l'augmentation automatique du nombre de pods de 3 √† 5 par la commande `kubectl scale`.
![alt text](assets/exercise02.png)

Enfin, on supprime un pod au hasard pour observer que Kubernetes recr√©e automatiquement un nouveau pod pour maintenir le nombre de r√©plicas d√©fini:

![alt text](assets/exercise02-1.png)

## Exercice 03 ‚Äî Services Kubernetes et acc√®s r√©seau

Objectif : Comprendre comment Kubernetes expose une application √† l‚Äôint√©rieur et √† l‚Äôext√©rieur du cluster en utilisant diff√©rents types de Services (`ClusterIP`, `NodePort`).

Maintenant que l'application NGINX est d√©ploy√©e, on va cr√©er un Service de type ClusterIP pour l'acc√®s interne, puis un Service de type NodePort pour l'acc√®s externe.

1. Cr√©ation d‚Äôun Service ClusterIP (interne) :
![alt text](assets/exercise03.png)

On remarque que le Service ClusterIP n‚Äôa pas d‚ÄôEXTERNAL-IP, car il est uniquement accessible √† l‚Äôint√©rieur du cluster.

On peut √©galement tester l‚Äôacc√®s interne depuis un pod en cr√©ant un pod temporaire avec l‚Äôimage alpine et en utilisant curl (ou wget) pour acc√©der au service NGINX via son nom DNS.

![alt text](assets/exercise03-1.png)

2. Cr√©ation d‚Äôun Service NodePort (externe) :
![alt text](assets/exercise03-3.png)

On remarque que le Service NodePort attribue un port externe (ici 31078) qui permet d‚Äôacc√©der √† l‚Äôapplication depuis l‚Äôext√©rieur du cluster. Par exemple, firefox sur la machine h√¥te avec l‚ÄôIP du n≈ìud et le port NodePort.

![alt text](assets/exercise03-2.png)

## Exercice 04 ‚Äî ConfigMaps & Secrets

Objectif : Apprendre √† g√©rer la configuration et les secrets dans Kubernetes afin que les applications restent d√©coupl√©es de leur configuration et que les informations sensibles soient prot√©g√©es.

Pour stocker des informations (configuration, mots de passe, cl√©s API) dans kubernetes, il existe deux objets principaux :
- ConfigMap : pour les informations non sensibles
- Secret : pour les informations sensibles

Dans cet exercice, on va tester les deux options pour stocker un message de bienvenue pour NGINX (ConfigMap) et un mot de passe (Secret).

1. Cr√©ation d‚Äôun ConfigMap

Cr√©ation du ConfigMap avec un message de bienvenue :
![alt text](assets/exercise04.png)

Ensuite, on cr√©e un pod qui utilise le ConfigMap pour injecter la variable d‚Äôenvironnement WELCOME_MESSAGE :
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-config-test
spec:
  containers:
  - name: nginx
    image: nginx
    env:
    - name: WELCOME_MESSAGE
      valueFrom:
        configMapKeyRef:
          name: nginx-config
          key: welcome_message
```

Puis on d√©ploie le pod et on v√©rifie que la variable d‚Äôenvironnement est bien d√©finie :
![alt text](assets/exercise04-1.png)

2. Cr√©ation d‚Äôun Secret

Cr√©ation du Secret avec un mot de passe encod√© en base64 :
![alt text](assets/exercise04-2.png)

Ensuite, on cr√©e un pod qui utilise le Secret pour injecter la variable d‚Äôenvironnement DB_PASSWORD :
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-secret-test
spec:
  containers:
  - name: nginx
    image: nginx
    env:
    - name: DB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: db-secret
          key: password
```

Puis on d√©ploie le pod et on v√©rifie que la variable d‚Äôenvironnement est bien d√©finie :
![alt text](assets/exercise04-3.png)

##¬†Exercice 05 ‚Äî Rolling Update

Objectif : Apprendre √† mettre √† jour un d√©ploiement Kubernetes sans interruption gr√¢ce aux rolling updates.

Les applications doivent souvent √™tre mises √† jour sans interrompre le service. Kubernetes permet de mettre √† jour les images d‚Äôun Deployment progressivement, un pod apr√®s l‚Äôautre.

Dans cet exercice, on va mettre √† jour l‚Äôimage NGINX d‚Äôun d√©ploiement existant vers une version plus r√©cente, tout en assurant une disponibilit√© continue.

1. V√©rification du d√©ploiement existant
![alt text](assets/exercise05.png)

2. Mise √† jour de l‚Äôimage du d√©ploiement et suivi de la mise √† jour
![alt text](assets/exercise05-1.png)

3. Rollback vers l'ancienne version
![alt text](assets/exercise05-2.png)

On peut voir que les pods sont mis √† jour progressivement, sans interruption de service. Kubernetes g√®re automatiquement le remplacement des pods et on peut revenir en arri√®re avec `rollout undo` si n√©cessaire.

## Exercice 06 ‚Äî Ingress

Objectif : Comprendre comment exposer plusieurs services Kubernetes via un point d‚Äôentr√©e unique avec Ingress.

NodePort permet d‚Äôexposer un service par port, mais si on a plusieurs services, c‚Äôest compliqu√©. Ingress permet de router le trafic HTTP/S vers diff√©rents services en fonction de l‚ÄôURL ou du nom d‚Äôh√¥te.

Dans cet exercice, on va installer un contr√¥leur Ingress (NGINX), puis cr√©er un Ingress pour router le trafic vers un service NGINX. Enfin, on v√©rifiera que les requ√™tes HTTP sont correctement rout√©es par Ingress.

1. Installation du contr√¥leur Ingress (NGINX)

On d√©ploie les ressources n√©cessaires pour Ingress NGINX
![alt text](assets/exercise06-2.png)

Puis on v√©rifie que tout s'est bien cr√©√©
![alt text](assets/exercise06-3.png)

2. Cr√©ation d‚Äôun Ingress pour le service NGINX

On cr√©e le contr√¥leur Ingress NGINX :
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
spec:
  ingressClassName: nginx
  rules:
  - host: example.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: nginx
            port:
              number: 80
```

Puis on applique le manifest et on v√©rifie que l‚ÄôIngress est cr√©√© :
![alt text](assets/exercise06-1.png)

3. V√©rification du routage via Ingress

Pour tester le routage, on ajoute une entr√©e dans le fichier `/etc/hosts` de notre machine locale pour faire pointer `example.local` vers l‚ÄôIP du n≈ìud (r√©cup√©r√©e via `kubectl get nodes -o wide`).

Ensuite, on effectue une requ√™te HTTP vers `example.local` :
![alt text](assets/exercise06.png)

On peut voir que la requ√™te est bien rout√©e vers le service NGINX via Ingress.

##¬†Exercice 07 ‚Äî Scaling & D√©ploiement avec Deployment

Objectif : Cr√©er un deployment √† partir d‚Äôun fichier YAML, comprendre le lien entre Deployment ‚Üí ReplicaSet ‚Üí Pods, et g√©rer le scaling.

Un Deployment permet de d√©clarer l‚Äô√©tat d√©sir√© d‚Äôune application (nombre de replicas, image, labels, etc.). Kubernetes s‚Äôoccupe de cr√©er et maintenir les pods correspondants. Les labels et le selector permettent de relier le Deployment aux pods qu‚Äôil g√®re.

Dans cet exercice, on va cr√©er un Deployment NGINX avec 3 r√©plicas, puis exposer le d√©ploiement via un Service NodePort, et enfin g√©rer le scaling en augmentant et diminuant le nombre de pods.

1. Cr√©ation du deployment

On commence par cr√©er un fichier web-deployment.yaml contenant :
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-logiciel # Le lien entre le deployment et les pods
  template:
    metadata:
      labels:
        app: nginx-logiciel # L'√©tiquette coll√©e sur chaque clone
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
```

Puis on le d√©ploie et on v√©rifie que les pods sont cr√©√©s :
![alt text](assets/exercise07.png)

2. Exposition du deployment via NodePort

On expose le d√©ploiement via un Service NodePort et on v√©rifie le service cr√©√© :
![alt text](assets/exercise07-1.png)

On teste l‚Äôacc√®s √† l‚Äôapplication :
![alt text](assets/exercise07-2.png)

3. Gestion du scaling

On augmente le nombre de pods √† 5 :
![alt text](assets/exercise07-3.png)

On diminue le nombre de pods √† 2 :
![alt text](assets/exercise07-4.png)

On peut voir que Kubernetes ajuste automatiquement le nombre de pods en fonction du nombre de r√©plicas sp√©cifi√© dans le Deployment.

## Exercice 08 ‚Äî Service interne ClusterIP

Objectif: Apprendre √† exposer un Deployment uniquement √† l‚Äôint√©rieur du cluster avec un service de type ClusterIP, et v√©rifier que les pods du Deployment sont bien accessibles via ce service.

Dans cet exercice, on va cr√©er un service interne pour que les pods puissent communiquer entre eux ou avec d‚Äôautres pods sans exposer le service √† l‚Äôext√©rieur.

1. Cr√©ation du Service ClusterIP

On commence par cr√©er un fichier web-service.yaml avec le contenu suivant :
```yaml
apiVersion: v1
kind: Service
metadata:
  name: web-service-unique
spec:
  selector:
    app: nginx-logiciel # Il cherche les pods avec ce label (ceux de notre Deployment !)
  ports:
    - protocol: TCP
      port: 80          # Le port du Service
      targetPort: 80    # Le port du Pod
  type: ClusterIP       # IP interne uniquement
```

Puis on d√©ploie le service et on v√©rifie qu‚Äôil est cr√©√© :
![alt text](assets/exercise08.png)

2. Test de la connectivit√© interne

On cr√©e un pod temporaire pour tester l‚Äôacc√®s au service :
![alt text](assets/exercise08-1.png)

On voit bien la page d‚Äôaccueil de NGINX, ce qui confirme que le service ClusterIP fonctionne correctement.

3. V√©rification des endpoints

On liste les endpoints du service pour v√©rifier qu‚Äôils correspondent aux pods du Deployment :
![alt text](assets/exercise08-2.png)

On peut voir que les IPs des pods du Deployment sont bien list√©es comme endpoints du service. Cela confirme que le service ClusterIP est correctement li√© aux pods.

## Exercice 09 ‚Äî D√©ploiement avec ConfigMap comme volume

Objectif : Apprendre √† utiliser une ConfigMap pour stocker du contenu (ici une page HTML) et le monter dans un pod en tant que volume, afin que NGINX puisse servir ce contenu.

Dans cet exercice, on va cr√©er une ConfigMap contenant une page HTML personnalis√©e, puis on va cr√©er un Deployment NGINX qui monte cette ConfigMap en tant que volume. Ainsi, NGINX servira la page HTML d√©finie dans la ConfigMap.

1. Cr√©ation de la ConfigMap

On cr√©e un fichier index.html avec le contenu suivant :
```html
<h1>Bienvenue sur mon NGINX version ConfigMap !</h1>
<p>Ce contenu est servi directement depuis une ConfigMap.</p>
```

Puis on cr√©e la ConfigMap √† partir de ce fichier :
![alt text](assets/exercise09.png)

2. Cr√©ation du Deployment avec montage de la ConfigMap

On cr√©e un fichier web-deployment-configmap.yaml avec le contenu suivant :
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-logiciel
  template:
    metadata:
      labels:
        app: nginx-logiciel
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
        - name: html-storage
          mountPath: /usr/share/nginx/html/  # Nginx lira notre ConfigMap ici
      volumes:
      - name: html-storage
        configMap:
          name: web-html-config # Le nom de la ConfigMap cr√©√©e √† l'√©tape A
```

Puis on applique le d√©ploiement et on v√©rifie que les pods sont cr√©√©s et que le deployment est accessible:
![alt text](assets/exercise09-1.png)

3. Mise √† jour de la ConfigMap

On modifie le fichier index.html pour changer le message de bienvenue, on met √† jour la ConfigMap, puis on red√©marre les pods pour qu‚Äôils prennent en compte la nouvelle version de la ConfigMap.:
![alt text](assets/exercise09-2.png)

Enfin, on v√©rifie que la nouvelle page est bien servie par NGINX :
![alt text](assets/exercise09-3.png)

On peut voir que NGINX sert bien le contenu mis √† jour depuis la ConfigMap, d√©montrant ainsi comment utiliser une ConfigMap comme volume dans un pod Kubernetes.

## Exercice 10 ‚Äî D√©ployer MariaDB avec volume persistant

Objectif : Apprendre √† d√©ployer une base de donn√©es MariaDB sur Kubernetes en utilisant un PersistentVolume (PV) et un PersistentVolumeClaim (PVC) pour le stockage persistant des donn√©es.

Dans cet exercice, on va cr√©er un PV utilisant un hostPath pour le stockage local sur le n≈ìud, un PVC pour r√©clamer ce stockage, puis d√©ployer MariaDB en utilisant ce PVC pour stocker ses donn√©es. L'objectif est de s'assurer que les donn√©es de la base persistent m√™me si le pod MariaDB est red√©marr√© ou recr√©√©.

1. Cr√©ation du secret pour les identifiants MariaDB

MariaDB n√©cessite un mot de passe pour l'utilisateur root. On va cr√©er un secret Kubernetes pour stocker ce mot de passe de mani√®re s√©curis√©e.
![alt text](assets/exercise10.png)

2. Cr√©ation du PersistentVolume (PV)

Le PV d√©finit la capacit√© de stockage et le chemin sur le n≈ìud o√π les donn√©es seront stock√©es. On cr√©e un fichier mariadb-pv.yaml avec le contenu suivant :
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mariadb-pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data-mariadb"
```

Puis on applique le PV et on v√©rifie qu‚Äôil est cr√©√© :
![alt text](assets/exercise10-1.png)

Avec ce PV, les donn√©es de MariaDB seront stock√©es localement sur le n≈ìud dans le r√©pertoire `/mnt/data-mariadb`. Ce type de volume est p√©dagogique, mais pas recommand√© en production car il ne supporte pas la haute disponibilit√© ni la portabilit√© des donn√©es entre n≈ìuds. De plus, comme le deployment force l'ex√©cution sur k8s-worker1, le r√©pertoire `/mnt/data-mariadb` doit √™tre cr√©√© manuellement sur ce n≈ìud avant le d√©ploiement.
![alt text](assets/exercise10-2.png)

3. Cr√©ation du PersistentVolumeClaim (PVC)

Le PVC permet de r√©clamer une partie du stockage d√©fini par le PV. On cr√©e un fichier mariadb-pvc.yaml avec le contenu suivant :
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mariadb-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
```

Puis on applique le PVC et on v√©rifie qu‚Äôil est cr√©√© :
![alt text](assets/exercise10-3.png)

4. D√©ploiement de MariaDB

Maintenant qu‚Äôon a le PV et le PVC, on peut d√©ployer MariaDB en utilisant le PVC pour le stockage des donn√©es. On cr√©e un fichier mariadb-deploy.yaml avec le contenu suivant :
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mariadb
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mariadb
  template:
    metadata:
      labels:
        app: mariadb
    spec:
      nodeSelector:
        kubernetes.io/hostname: k8s-worker1
      containers:
      - name: mariadb
        image: mariadb:10.6
        env:
        - name: MARIADB_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mariadb-pass
              key: password
        - name: MARIADB_DATABASE
          value: mabase
        ports:
        - containerPort: 3306
        volumeMounts:
        - name: storage
          mountPath: /var/lib/mysql
      volumes:
      - name: storage
        persistentVolumeClaim:
          claimName: mariadb-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: mariadb-service
spec:
  selector:
    app: mariadb
  ports:
    - port: 3306
      targetPort: 3306
```

Puis on applique le d√©ploiement et on v√©rifie que le pod MariaDB est cr√©√© et en cours d‚Äôex√©cution :
![alt text](assets/exercise10-4.png)

5. Test de connectivit√© √† MariaDB

On cr√©e un pod temporaire pour tester la connexion √† MariaDB :
![alt text](assets/exercise10-5.png)

6. V√©rification de la persistance des donn√©es

Enfin, on va v√©rifier que les donn√©es persistent m√™me apr√®s le red√©marrage du pod MariaDB.

On commence par cr√©er une base de donn√©es et une table et on ins√®re des donn√©es :
![alt text](assets/exercise10-6.png)

Ensuite, on supprime le pod MariaDB pour simuler un red√©marrage et on v√©rifie que le pod est recr√©√© automatiquement par le deployment :
![alt text](assets/exercise10-7.png)

Apr√®s le red√©marrage, on se reconnecte √† MariaDB et on v√©rifie que les donn√©es ins√©r√©es pr√©c√©demment sont toujours pr√©sentes :
![alt text](assets/exercise10-8.png)

On peut voir que les donn√©es ins√©r√©es avant le red√©marrage du pod sont toujours pr√©sentes, ce qui confirme que le stockage persistant via le PersistentVolume et le PersistentVolumeClaim fonctionne correctement avec MariaDB sur Kubernetes.

## Exercice 11 ‚Äî D√©ployer une application PHP connect√©e √† MariaDB

Objectif : D√©ployer une application PHP simple sur Kubernetes qui se connecte √† une base de donn√©es MariaDB, en utilisant des ConfigMaps et des Secrets pour la configuration.

Dans cet exercice, on va cr√©er une ConfigMap pour le code PHP, un Secret pour le mot de passe de la base de donn√©es, puis d√©ployer une application PHP qui se connecte √† MariaDB.

On suppose que MariaDB est d√©j√† d√©ploy√©e et accessible via le service `mariadb-service`.

1. Cr√©ation de la ConfigMap pour le code PHP

On cr√©e un fichier index.php avec le contenu suivant :
```php
apiVersion: v1
kind: ConfigMap
metadata:
  name: php-code
data:
  index.php: |
    <?php
    $host = 'mariadb-service';
    $db   = 'mabase';
    $user = 'root';
    $pass = getenv('DB_PASSWORD'); // üîê Mot de passe depuis le Secret

    try {
        $dsn = "mysql:host=$host;dbname=$db;charset=utf8mb4";
        $pdo = new PDO($dsn, $user, $pass);

        echo "<body style='font-family:sans-serif; text-align:center; padding-top:50px; background-color:#f0fff4;'>";
        echo "<h1 style='color:#2f855a;'>‚úÖ Connexion R√©ussie !</h1>";
        echo "<p>PHP communique correctement avec MariaDB.</p>";
        echo "<p><b>Service DB :</b> $host</p>";
        echo "<p><b>IP Pod PHP :</b> " . $_SERVER['SERVER_ADDR'] . "</p>";
        echo "</body>";
    } catch (PDOException $e) {
        echo "<body style='font-family:sans-serif; text-align:center; padding-top:50px; background-color:#fff5f5;'>";
        echo "<h1 style='color:#c53030;'>‚ùå Erreur de Connexion</h1>";
        echo "<p>" . $e->getMessage() . "</p>";
        echo "</body>";
    }
    ?>
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: php-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: php-web
  template:
    metadata:
      labels:
        app: php-web
    spec:
      containers:
      - name: php
        image: php:8.0-apache
        command: ["sh", "-c", "docker-php-ext-install pdo pdo_mysql && apache2-foreground"]
        ports:
        - containerPort: 80
        env:
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mariadb-pass
              key: password
        volumeMounts:
        - name: code-volume
          mountPath: /var/www/html/index.php
          subPath: index.php
      volumes:
      - name: code-volume
        configMap:
          name: php-code
---
apiVersion: v1
kind: Service
metadata:
  name: php-service
spec:
  type: ClusterIP
  selector:
    app: php-web
  ports:
    - port: 80
      targetPort: 80
```

Puis on applique la ConfigMap :
![alt text](assets/exercise11.png)

2. V√©rification de la ConfigMap

Et on v√©rifie qu‚Äôelle est cr√©√©e :
![alt text](assets/exercise11-1.png)

On voit sur le screenshot ci-dessus que la ConfigMap contient bien le code PHP.

### Activit√© 1: Compr√©hension de l'architecture

A partir des manifestes fournis, on va analyser l'architecture de l'application PHP connect√©e √† MariaDB, et identifier les diff√©rents composants et leur r√¥le. On r√©alisera √©galement un sch√©ma simple de l'architecture.


On utilise quelques commandes basiques pour explorer les ressources cr√©√©es :
![alt text](assets/exercise11-2.png)

On en d√©duit:
- **Le pod MariaDB :** `mariadb-6bdb6b75c-6qfc2` (IP: `10.244.3.25`, **Node :** `k8s-worker1`)
- **Les pods PHP :** `php-app-569f46cdf7-hz84s` (IP: `10.244.1.22`, **node** `k8s-worker3`) et `php-app-569f46cdf7-tfps4` (IP: `10.244.2.38`, **node** `k8s-worker2`)
- **Les services :**
  - `mariadb-service` ‚Äî **ClusterIP** `10.101.80.222` (port 3306) ‚Üí point d‚Äôacc√®s DB interne.
  - `php-service` ‚Äî **ClusterIP** `10.105.190.224` (port 80) ‚Üí service interne pour l‚Äôapp PHP.
  - `nginx` ‚Äî **NodePort** `10.102.91.113` (80:31506) ‚Üí expos√© sur les n≈ìuds.
  - `web-deployment` ‚Äî **NodePort** `10.107.20.178` (80:32176).
  - `web-service-unique` ‚Äî **ClusterIP** `10.111.171.64` (port 80).
- **Les volumes utilis√©s :**
  - PersistentVolume **`mariadb-pv`** (1Go) ‚Äî **li√©** √† PersistentVolumeClaim **`mariadb-pvc`**. Ce stockage permet la persistance des donn√©es MariaDB.
- **Les Secrets & ConfigMaps :**
  - **Secrets :** `mariadb-pass` (contient la cl√© `password` utilis√©e par MariaDB), `db-secret`, `regcred` (docker registry cred).
  - **ConfigMaps :** `php-code` (contient `index.php` mont√© dans les pods PHP), `nginx-config`, `web-html-config`, `kube-root-ca.crt`.

Diff√©rence entre Service et Pod:
- Un **Pod** est l'unit√© de base de d√©ploiement dans Kubernetes, repr√©sentant un ou plusieurs conteneurs qui partagent le m√™me r√©seau et stockage.
- Un **Service** est une abstraction qui d√©finit un ensemble logique de pods et une politique d'acc√®s √† ces pods, permettant la d√©couverte et la communication entre eux.

Diff√©rence entre ConfigMap et Secret:
- Une **ConfigMap** est utilis√©e pour stocker des donn√©es de configuration non sensibles sous forme de paires cl√©-valeur.
- Un **Secret** est utilis√© pour stocker des informations sensibles, telles que des mots de passe ou des cl√©s API, de mani√®re s√©curis√©e.

### Activit√© 2: V√©rification de la connexion r√©seau

Depuis un Pod PHP (`php-app`), on v√©rifie la r√©solution DNS et la connectivit√© vers le service `mariadb-service` : le nom se r√©sout correctement et la connexion MySQL est fonctionnelle.

On utilise `mariadb-service` plut√¥t qu'une IP parce que le Service fournit un point d'acc√®s stable (DNS + IP virtuelle) et effectue du load‚Äëbalancing vers les pods backend ; les IP des pods sont √©ph√©m√®res et peuvent changer lors des red√©ploiements ou d√©placements des pods.

### Activit√© 3: Cr√©ation manuelle de donn√©es

On cr√©e une table posts et on ajoute quelques entr√©es manuellement via le pod mariadb.

![alt text](assets/exercise11-3.png)

### Activit√© 4: Test de la persistance des donn√©es

On va supprimer le pod MariaDB pour simuler un red√©marrage, puis v√©rifier que les donn√©es ins√©r√©es pr√©c√©demment sont toujours pr√©sentes.

![alt text](assets/exercise11-4.png)

On voit sur le screenshot que les donn√©es ins√©r√©es avant le red√©marrage du pod sont toujours pr√©sentes, ce qui confirme que le stockage persistant via le PersistentVolume et le PersistentVolumeClaim fonctionne correctement avec MariaDB sur Kubernetes.

Le volume persistant permet de conserver les donn√©es m√™me si le pod est supprim√© ou red√©marr√©, assurant ainsi la durabilit√© des donn√©es de la base MariaDB.

### Activit√© 5: Modifier le code PHP via ConfigMap

Nombre d'utilisateurs dans la table:
![alt text](assets/exercise11-5.png)

On met maintenant √† jour la ConfigMap `php-app` pour modifier le code PHP.

On applique la nouvelle ConfigMap et on red√©marre les pods PHP pour qu‚Äôils prennent en compte la nouvelle version du code :

![alt text](assets/exercise11-6.png)

### Activit√© 6: Simulation de panne applicative

On va simuler une panne de l‚Äôapplication PHP en supprimant un pod PHP, puis v√©rifier que Kubernetes recr√©e automatiquement un nouveau pod pour maintenir le nombre de r√©plicas d√©fini dans le Deployment.

![alt text](assets/exercise11-7.png)

Les r√©plicas sont bien maintenus √† 2, et le nouveau pod recr√©√© fonctionne correctement.

Les r√©plicas assurent la haute disponibilit√© de l‚Äôapplication en maintenant un nombre constant de pods en cours d‚Äôex√©cution. Si un pod √©choue ou est supprim√©, Kubernetes cr√©e automatiquement un nouveau pod pour remplacer celui qui a √©t√© perdu, garantissant ainsi que l‚Äôapplication reste disponible et fonctionnelle.

###¬†Activit√© 7: D√©bogage kubernetes

### Activit√© 7 ‚Äì D√©bogage Kubernetes (bonus ‚≠ê)

Dans cette activit√©, on va volontairement introduire des erreurs dans la configuration de l‚Äôapplication PHP connect√©e √† MariaDB, puis utiliser les outils de diagnostic Kubernetes pour identifier et comprendre les probl√®mes.

#### 1. Erreur : mauvais nom de service

On modifie la ConfigMap contenant le code PHP pour utiliser un nom de service inexistant (par exemple `mariadb-service-faux` au lieu de `mariadb-service`). Apr√®s avoir red√©marr√© les pods PHP, l‚Äôapplication ne parvient plus √† se connecter √† la base de donn√©es.

Pour diagnostiquer l‚Äôerreur :
```bash
kubectl logs <nom-du-pod-php>
```
On observe un message d‚Äôerreur indiquant l‚Äôimpossibilit√© de r√©soudre le nom DNS ou d‚Äô√©tablir la connexion √† la base.

On peut √©galement utiliser :
```bash
kubectl describe pod <nom-du-pod-php>
```
pour v√©rifier les √©v√©nements r√©cents et les √©ventuelles erreurs de d√©marrage.

#### 2. Erreur : mauvais mot de passe DB

On modifie le Secret `mariadb-pass` pour y mettre un mot de passe incorrect, puis on red√©marre les pods PHP. L‚Äôapplication √©choue √† se connecter √† MariaDB.

Diagnostic :
```bash
kubectl logs <nom-du-pod-php>
```
affiche une erreur d‚Äôauthentification MySQL (ex : `Access denied for user 'root'@'...'`).

#### Questions de r√©flexion

- **Pourquoi ne pas mettre le mot de passe DB dans le code PHP ?**
  
  Mettre le mot de passe en dur dans le code PHP expose un risque de s√©curit√© : toute personne ayant acc√®s au code source peut lire le mot de passe. Utiliser un Secret Kubernetes permet de s√©parer les informations sensibles du code et de limiter leur exposition.

- **Quelle diff√©rence entre red√©marrer un Pod et red√©ployer une application ?**
  
  Red√©marrer un Pod (par exemple en le supprimant) entra√Æne la recr√©ation d‚Äôun pod identique par le Deployment, sans modification de la configuration ou de l‚Äôimage. Red√©ployer une application (par exemple apr√®s modification du manifest ou de l‚Äôimage) met √† jour la configuration, l‚Äôimage ou les variables d‚Äôenvironnement, et peut entra√Æner la cr√©ation de nouveaux pods avec ces changements.

- **Que se passe-t-il si le n≈ìud Kubernetes tombe ?**
  
  Si un n≈ìud tombe, tous les pods qui y √©taient h√©berg√©s deviennent indisponibles. Kubernetes d√©tecte l‚Äôindisponibilit√© du n≈ìud et recr√©e automatiquement les pods sur les autres n≈ìuds disponibles (si les ressources le permettent), assurant ainsi la haute disponibilit√© de l‚Äôapplication.
